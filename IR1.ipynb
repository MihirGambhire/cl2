{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program for pre-processing of a text document such as stop word removal, stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sayal\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Program Files\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sayal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sayal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sayal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING PRE-PROCESSING (Stemming & Lemmatization) ---\n",
      "\n",
      "Original: in a small village nestled between rolling hills and ancient...\n",
      "  STEMMED : ['small', 'villag', 'nestl', 'roll', 'hill', 'ancient', 'forest', 'mysteri', 'old', 'librari', 'seem', 'hold', 'secret', 'world', 'librari', 'built', 'weather', 'stone', 'cover', 'ivi', 'place', 'time', 'felt', 'though', 'stood', 'still', 'shelv', 'line', 'book', 'shape', 'size', 'ancient', 'page', 'threaten', 'crumbl', 'slightest', 'touch']\n",
      "  LEMMATIZED : ['small', 'village', 'nestle', 'roll', 'hill', 'ancient', 'forest', 'mysterious', 'old', 'library', 'seem', 'hold', 'secrets', 'world', 'library', 'build', 'weather', 'stone', 'cover', 'ivy', 'place', 'time', 'felt', 'though', 'stand', 'still', 'shelve', 'line', 'book', 'shape', 'size', 'ancient', 'page', 'threaten', 'crumble', 'slightest', 'touch']\n",
      "\n",
      "Original: one day, a young girl named elara stumbled upon this hidden ...\n",
      "  STEMMED : ['one', 'day', 'young', 'girl', 'name', 'elara', 'stumbl', 'upon', 'hidden', 'treasur', 'elara', 'curiou', 'adventur', 'soul', 'alway', 'seek', 'new', 'stori', 'knowledg', 'wander', 'aisl', 'discov', 'dusti', 'old', 'book', 'cover', 'adorn', 'strang', 'symbol', 'titl', 'written', 'languag', 'recogn', 'moment', 'open', 'gust', 'wind', 'swirl', 'around', 'word', 'page', 'began', 'glow']\n",
      "  LEMMATIZED : ['one', 'day', 'young', 'girl', 'name', 'elara', 'stumble', 'upon', 'hide', 'treasure', 'elara', 'curious', 'adventurous', 'soul', 'always', 'seek', 'new', 'stories', 'knowledge', 'wander', 'aisles', 'discover', 'dusty', 'old', 'book', 'cover', 'adorn', 'strange', 'symbols', 'title', 'write', 'language', 'recognize', 'moment', 'open', 'gust', 'wind', 'swirl', 'around', 'word', 'page', 'begin', 'glow']\n",
      "\n",
      "Original: the book spoke of a forgotten era, of lost civilizations and...\n",
      "  STEMMED : ['book', 'spoke', 'forgotten', 'era', 'lost', 'civil', 'magic', 'realm', 'beyond', 'imagin', 'told', 'tale', 'brave', 'hero', 'power', 'sorcer', 'epic', 'battl', 'good', 'evil', 'elara', 'captiv', 'eye', 'wide', 'wonder', 'devour', 'stori', 'knew', 'found', 'someth', 'extraordinari', 'someth', 'could', 'chang', 'life', 'forev']\n",
      "  LEMMATIZED : ['book', 'speak', 'forget', 'era', 'lose', 'civilizations', 'magical', 'realms', 'beyond', 'imagination', 'tell', 'tales', 'brave', 'heroes', 'powerful', 'sorcerers', 'epic', 'battle', 'good', 'evil', 'elara', 'captivate', 'eye', 'wide', 'wonder', 'devour', 'story', 'know', 'find', 'something', 'extraordinary', 'something', 'could', 'change', 'life', 'forever']\n",
      "\n",
      "Original: little did she know, the library had chosen her for a reason...\n",
      "  STEMMED : ['littl', 'know', 'librari', 'chosen', 'reason', 'elara', 'embark', 'adventur', 'unlik', 'guid', 'wisdom', 'magic', 'contain', 'within', 'ancient', 'book', 'first', 'ray', 'dawn', 'broke', 'stain', 'glass', 'window', 'elara', 'step', 'readi', 'explor', 'world', 'uncov', 'secret', 'hidden', 'centuri']\n",
      "  LEMMATIZED : ['little', 'know', 'library', 'choose', 'reason', 'elara', 'embark', 'adventure', 'unlike', 'guide', 'wisdom', 'magic', 'contain', 'within', 'ancient', 'book', 'first', 'ray', 'dawn', 'break', 'stain', 'glass', 'windows', 'elara', 'step', 'ready', 'explore', 'world', 'uncover', 'secrets', 'hide', 'centuries']\n",
      "\n",
      "\n",
      "--- PROCESS COMPLETE ---\n",
      "\n",
      "Final list of all *stemmed* tokens (for CSV):\n",
      "['small', 'villag', 'nestl', 'roll', 'hill', 'ancient', 'forest', 'mysteri', 'old', 'librari', 'seem', 'hold', 'secret', 'world', 'librari', 'built', 'weather', 'stone', 'cover', 'ivi', 'place', 'time', 'felt', 'though', 'stood', 'still', 'shelv', 'line', 'book', 'shape', 'size', 'ancient', 'page', 'threaten', 'crumbl', 'slightest', 'touch', 'one', 'day', 'young', 'girl', 'name', 'elara', 'stumbl', 'upon', 'hidden', 'treasur', 'elara', 'curiou', 'adventur', 'soul', 'alway', 'seek', 'new', 'stori', 'knowledg', 'wander', 'aisl', 'discov', 'dusti', 'old', 'book', 'cover', 'adorn', 'strang', 'symbol', 'titl', 'written', 'languag', 'recogn', 'moment', 'open', 'gust', 'wind', 'swirl', 'around', 'word', 'page', 'began', 'glow', 'book', 'spoke', 'forgotten', 'era', 'lost', 'civil', 'magic', 'realm', 'beyond', 'imagin', 'told', 'tale', 'brave', 'hero', 'power', 'sorcer', 'epic', 'battl', 'good', 'evil', 'elara', 'captiv', 'eye', 'wide', 'wonder', 'devour', 'stori', 'knew', 'found', 'someth', 'extraordinari', 'someth', 'could', 'chang', 'life', 'forev', 'littl', 'know', 'librari', 'chosen', 'reason', 'elara', 'embark', 'adventur', 'unlik', 'guid', 'wisdom', 'magic', 'contain', 'within', 'ancient', 'book', 'first', 'ray', 'dawn', 'broke', 'stain', 'glass', 'window', 'elara', 'step', 'readi', 'explor', 'world', 'uncov', 'secret', 'hidden', 'centuri']\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(text):\n",
    "\n",
    "    \n",
    "    # Get tools ready\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # Process the text\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.isalpha() and word not in stopwords_list:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    # Now, process the filtered list in two ways\n",
    "    stemmed_words = [ps.stem(word) for word in filtered_words]\n",
    "    lemmatized_words = [wnl.lemmatize(word, pos='v') for word in filtered_words]\n",
    "\n",
    "\n",
    "    print(f\"Original: {text[:60]}...\")\n",
    "    print(f\"  STEMMED : {stemmed_words}\")\n",
    "    print(f\"  LEMMATIZED : {lemmatized_words}\\n\")\n",
    "\n",
    "    # Return the stemmed words for the main practical requirement\n",
    "    return stemmed_words\n",
    "\n",
    "# --- 4. File Processing ---\n",
    "processed_text = []\n",
    "file_name = \"sample.txt\" # Make sure this file is in the same directory\n",
    "\n",
    "print(\"--- STARTING PRE-PROCESSING (Stemming & Lemmatization) ---\\n\")\n",
    "try:\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # Skip empty lines\n",
    "                # The function will print both, but only returns the stemmed words\n",
    "                stemmed_tokens = pre_processing(line)\n",
    "                processed_text.extend(stemmed_tokens)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_name}' was not found.\")\n",
    "print(\"\\n--- PROCESS COMPLETE ---\")\n",
    "\n",
    "# --- 5. Print Final List (Stemmed) ---\n",
    "print(\"\\nFinal list of all *stemmed* tokens (for CSV):\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'villag', 'nestl', 'roll', 'hill', 'ancient', 'forest', 'mysteri', 'old', 'librari', 'seem', 'hold', 'secret', 'world', 'librari', 'built', 'weather', 'stone', 'cover', 'ivi', 'place', 'time', 'felt', 'though', 'stood', 'still', 'shelv', 'line', 'book', 'shape', 'size', 'ancient', 'page', 'threaten', 'crumbl', 'slightest', 'touch', 'one', 'day', 'young', 'girl', 'name', 'elara', 'stumbl', 'upon', 'hidden', 'treasur', 'elara', 'curiou', 'adventur', 'soul', 'alway', 'seek', 'new', 'stori', 'knowledg', 'wander', 'aisl', 'discov', 'dusti', 'old', 'book', 'cover', 'adorn', 'strang', 'symbol', 'titl', 'written', 'languag', 'recogn', 'moment', 'open', 'gust', 'wind', 'swirl', 'around', 'word', 'page', 'began', 'glow', 'book', 'spoke', 'forgotten', 'era', 'lost', 'civil', 'magic', 'realm', 'beyond', 'imagin', 'told', 'tale', 'brave', 'hero', 'power', 'sorcer', 'epic', 'battl', 'good', 'evil', 'elara', 'captiv', 'eye', 'wide', 'wonder', 'devour', 'stori', 'knew', 'found', 'someth', 'extraordinari', 'someth', 'could', 'chang', 'life', 'forev', 'littl', 'know', 'librari', 'chosen', 'reason', 'elara', 'embark', 'adventur', 'unlik', 'guid', 'wisdom', 'magic', 'contain', 'within', 'ancient', 'book', 'first', 'ray', 'dawn', 'broke', 'stain', 'glass', 'window', 'elara', 'step', 'readi', 'explor', 'world', 'uncov', 'secret', 'hidden', 'centuri']\n"
     ]
    }
   ],
   "source": [
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"output.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerow(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of the Practical\n",
    "The purpose of this practical is to clean and normalize raw text data. Computers can't understand sentences directly. They see \"running\" and \"ran\" as two completely different words.\n",
    "\n",
    "Pre-processing cleans this \"unstructured\" data by:\n",
    "\n",
    "Tokenizing: Splitting sentences into a list of individual words (tokens).\n",
    "\n",
    "Removing Noise: Getting rid of common \"filler\" words (like 'a', 'the', 'is') and punctuation that don't add much meaning.\n",
    "\n",
    "Normalizing: Reducing words to their common root (e.g., \"running\" -> \"run\").\n",
    "\n",
    "This makes the data much smaller, more efficient, and easier for analysis algorithms (like search engines or spam filters) to understand.\n",
    "\n",
    "üß† Core Theory (How it Works)\n",
    "There are two main techniques you're using to normalize words:\n",
    "\n",
    "Stemming (using PorterStemmer): This is a crude, rule-based process. It simply chops off the end of words to get to a common \"stem.\"\n",
    "\n",
    "It's very fast.\n",
    "\n",
    "It's \"aggressive\" and the resulting stem might not be a real word (e.g., \"studies\" becomes \"studi\", \"history\" becomes \"histori\").\n",
    "\n",
    "Example: studies, studying, study all become studi.\n",
    "\n",
    "Lemmatization (using WordNetLemmatizer): This is a \"smarter,\" dictionary-based process. It uses the WordNet dictionary to find the actual root word (the \"lemma\") based on its part of speech (like a verb or noun).\n",
    "\n",
    "It's slower (it has to look up words).\n",
    "\n",
    "It's more accurate and the result is always a real word.\n",
    "\n",
    "Example: studies, studying, study all become study.\n",
    "\n",
    "üìã Step-by-Step Code Explanation\n",
    "Import Libraries: You first import nltk (Natural Language Toolkit) and the specific tools you need:\n",
    "\n",
    "word_tokenize: To split sentences into words.\n",
    "\n",
    "stopwords: To get the list of \"filler\" words.\n",
    "\n",
    "PorterStemmer: The tool for stemming.\n",
    "\n",
    "WordNetLemmatizer: The tool for lemmatizing.\n",
    "\n",
    "Download NLTK Data: The nltk.download() commands are essential. They download the required model for 'punkt' (tokenization), the 'stopwords' list, and the 'wordnet' dictionary.\n",
    "\n",
    "Define pre_processing Function:\n",
    "\n",
    "Initialize Tools: You create instances of the PorterStemmer, WordNetLemmatizer, and load the list of stopwords.words('english') into a set() (which is much faster for lookups than a list).\n",
    "\n",
    "Tokenize & Filter: The code takes a line of text, converts it to .lower(), and uses word_tokenize() to get a list. It then loops through this list and keeps only the words that are .isalpha() (to remove punctuation and numbers) and are not in the stopword list.\n",
    "\n",
    "Stem & Lemmatize: It then processes this filtered list twice:\n",
    "\n",
    "Once with ps.stem(word) to create the stemmed list.\n",
    "\n",
    "Once with wnl.lemmatize(word, pos='v') to create the lemmatized list.\n",
    "\n",
    "Process the File:\n",
    "\n",
    "The main part of the script opens your sample.txt file.\n",
    "\n",
    "It reads the file line by line.\n",
    "\n",
    "It calls your pre_processing function on each line.\n",
    "\n",
    "It .extend()s a final list (processed_text) with the stemmed tokens returned from the function.\n",
    "\n",
    "Save Output: Finally, it uses the csv library to save your final list of stemmed tokens into the output.csv file.\n",
    "\n",
    "üõ†Ô∏è Key Libraries & Functions\n",
    "nltk: The main library for all Natural Language Processing (NLP) in Python.\n",
    "\n",
    "word_tokenize(text): Splits a string into a list of tokens (words and punctuation).\n",
    "\n",
    "stopwords.words('english'): The built-in list of common English stop words.\n",
    "\n",
    "word.isalpha(): A simple string method that returns True if all characters are letters. This is an easy way to remove numbers and punctuation (like '!', ',', '.') at the same time.\n",
    "\n",
    "PorterStemmer(): The most common stemmer. You call its .stem(word) method.\n",
    "\n",
    "WordNetLemmatizer(): The NLTK lemmatizer. You call its .lemmatize(word, pos='v') method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
